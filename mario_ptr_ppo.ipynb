{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19939a9c-03f0-4150-9477-9109e9735319",
      "metadata": {
        "id": "19939a9c-03f0-4150-9477-9109e9735319"
      },
      "source": [
        "# Install librarys\n",
        "Install packages need to train mario agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
      "metadata": {
        "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install gym==0.25.2\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install imageio\n",
        "# !pip install torchvision\n",
        "!pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Qz1YrCan2cn",
      "metadata": {
        "id": "3Qz1YrCan2cn"
      },
      "source": [
        "# import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f",
      "metadata": {
        "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import random, os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "#import multiprocessing as mp\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beCQDbAxn45P",
      "metadata": {
        "id": "beCQDbAxn45P"
      },
      "source": [
        "# Create hyperparammeters\n",
        "Config hyperparammeters, just change it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a95999c-5111-4054-9256-45cd206a697e",
      "metadata": {
        "id": "4a95999c-5111-4054-9256-45cd206a697e"
      },
      "outputs": [],
      "source": [
        "#class DictWrapper create by Chatgpt\n",
        "class DictWrapper:\n",
        "    def __init__(self, dictionary):\n",
        "        self._dict = dictionary\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item in self._dict:\n",
        "            return self._dict[item]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DictWrapper' object has no attribute '{item}'\")\n",
        "\n",
        "config = {\n",
        "    'num_envs': 16,\n",
        "    'save_model_step': int(1e5),\n",
        "    'save_figure_step': 512,\n",
        "    'learn_step': 512,\n",
        "    'total_step_or_episode': 'step',\n",
        "    'total_step': int(5e6),\n",
        "    'total_episode': None,\n",
        "    'batch_size': 64,\n",
        "    'save_dir': \"\",\n",
        "    'gamma': 0.9,\n",
        "    'learning_rate': 7e-5,\n",
        "    'state_dim': (4, 84, 84),\n",
        "    'action_dim': 12,#12 for complex, 7 for simple\n",
        "    'entropy_coef': 0.01,\n",
        "    'V_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'clip_param': 0.2,\n",
        "    'num_epoch': 2,\n",
        "    'world': 1,\n",
        "    'stage': 1,\n",
        "    'action_type': 'complex',\n",
        "    'is_normalize_advantage': False,\n",
        "    'V_loss_type': \"huber\", #\"mse\"\n",
        "    'target_kl': 0.05,\n",
        "    'gae_lambda': 0.95,\n",
        "    'per_buffer_size': 256,\n",
        "    'per_eps': 1e-2,\n",
        "    'per_alpha': 0.7,\n",
        "    'per_beta': 0.4,\n",
        "    'eps_marg': 0.2,\n",
        "    'off_epoch': 8,\n",
        "}\n",
        "\n",
        "config = DictWrapper(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ha80HrVm1_OR",
      "metadata": {
        "id": "ha80HrVm1_OR"
      },
      "outputs": [],
      "source": [
        "gym.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aH9EB8pQn-ao",
      "metadata": {
        "id": "aH9EB8pQn-ao"
      },
      "source": [
        "# Define environment\n",
        "## Create a custom environment, We need:\n",
        "- SkipFrame: because the episode is very long then in this environment, we only need to repeat action sometimes. Then we repeat each action 4 times and skip 3 first frames (return 4th frame). We need to sum 4 rewards from 4 frames.\n",
        "- GrayScaleResizeObservation: grayscale state (from RGB to gray image) and resize to 84x84.\n",
        "- NoopResetEnv: when resetting the environment, I do random actions sometimes before using the evironment. This same as atari strategy. When reset, we choice num_noops random actions by random from 0 to noop_max and random do num_noops actions. If random actions yield to a terminal state, then reset and continue do random action. I set noop_max as 30 same as atari.\n",
        "- CustomRewardAndDoneEnv\n",
        "    - I see that a lot of people train Mario to use this custom reward, and then I copy it. Just add 50 rewards if the agent solves the stage and add -50 rewards if the agent is dead. And the reward is divided into 10. I set done to True if Mario dies instead of when Mario lost all lives as default.\n",
        "    - stage 4-2: give a -50 reward when Mario moves on top of the map (y_pos >= 255).\n",
        "    - stages 4-4 and 7-4: set done = True when Mario goes to wrong way and gives a -50 penalty reward. If Mario goes to the true way but the map still loops (this is a bug), I set done = True but don't give a penalty reward.\n",
        "    - stage 8-4: set done = True when Mario goes to wrong way and gives a -50 penalty reward (like 4-4 and 7-4), but this map have 1 hardest part that Mario need to find hidden brick. I set penalty reward as -100 for this part (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500).\n",
        "    - stages 4-4 and 8-4: give a -0.1 reward every step.\n",
        "\n",
        "\n",
        "## About reward system:\n",
        "- set done to True when Mario dies: This is the most important thing because, in the default reward system, Mario still yields a reward by just moving right, if Mario dies, the agent can't lose total rewards and the agent still go right (in new life) and get more rewards. This is the easiest way to get rewards and the agent also learns this trick.\n",
        "- penalty -50 reward when Mario dies: this is necessary to make Mario train faster. If we don't add a penalty, the agent can't complete hard stages.\n",
        "reward 50 when reaching the flag: this makes Mario train faster and surpass stuck points in hard stages.\n",
        "- change penalty and reaching flag reward of more or less than 50 don't make more difference then I don't change it.\n",
        "- Divide rewards by 10: I think it makes total rewards smaller and the agent can learn a better way, but I am not sure how it is necessary. I just copy it.\n",
        "- With Stage 4-2: I see that the agent can reach more rewards when Mario goes to the wrap zone, but Mario can't win this stage with the wrap zone way because the reward system gives negative rewards when Mario goes left. Then I just give a penalty reward when Mario moves to the top of the map.\n",
        "- Use FrameStack to stack lastest 4 frames as observation\n",
        "- with stage 4-4 and 7-4:\n",
        "    - because this map has the wrong way, Mario will go to the loop and the reward still increase to infinity, then I need to set done = True and give a negative penalty reward\n",
        "    - negative reward to prevent Mario go to the wrong way.\n",
        "    - Another strategy is to give a negative reward but don't set done = True (like 4-2). But this map has a bug and this strangtegy not working.\n",
        "    - even though Mario goes in the correct way, sometimes Mario still goes in the loop. Then I set done = True every time Mario goes on the loop (check by x_pos and max_x_pos).\n",
        "- stage 4-4: give a -0.1 reward every step: prevent Mario stuck. This map have 1 part that Mario need to move left, but move left give negative reward in default system. Go right make Mario go to wrong way, than the episode ended and Mario get negative reward. Than Mario often stop moving to maintain rewards. To make Mario move, I need add negative reward every step.\n",
        "- stage 8-4:\n",
        "    - give a -0.1 reward every step: prevent Mario from getting stuck. This makes Mario move when they are stuck at part (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500). Because if Mario can't move right (go the wrong way and get negative reward), Agent often learn that they can't do anything to get more reward. Then agent often stop moving. This penalty reward makes agent still explore to find hidden brick.\n",
        "    - same as 4-4 and 7-4, I set done = True and give -50 penalty reward when Agent move in wrong way. But at hardest part (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500), I set penalty reward as -100.\n",
        "    - Another strategy is giving +50 reward when agent find hidden brick or agent go in true way (don't go the wrong way). I tried and all 3 ways work.\n",
        "    - Because the map has many locations with overlapping x_pos (especially the underwater part where x_pos is marked again from 1). You must be careful if changing the custom reward system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
      "metadata": {
        "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da"
      },
      "outputs": [],
      "source": [
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "print(env.action_space)\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "class GrayScaleResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.current_state = observation\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        observation = observation.astype(np.uint8)#.reshape(-1, observation.shape[0], observation.shape[1])\n",
        "        return observation\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        noops = np.random.randint(0, self.noop_max, (1, ))[0]\n",
        "        for _ in range(noops):\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, _, done, _, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        obs, reward, done, trunk, info = self.env.step(ac)\n",
        "        return obs, reward, done, trunk, info\n",
        "\n",
        "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, world=1, stage=1):\n",
        "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            self.sea_map = False\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, trunc, info = self.env.step(action)\n",
        "\n",
        "        if (info['x_pos'] - self.current_x) == 0:\n",
        "            self.current_x_count += 1\n",
        "        else:\n",
        "            self.current_x_count = 0\n",
        "        if info[\"flag_get\"]:\n",
        "            reward += 50\n",
        "            done = True\n",
        "        if done and info[\"flag_get\"] == False and info[\"time\"] != 0:\n",
        "            reward -= 50\n",
        "            done = True\n",
        "        self.current_x = info[\"x_pos\"]\n",
        "\n",
        "        if self.world == 7 and self.stage == 4:\n",
        "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
        "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
        "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
        "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
        "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
        "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
        "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "        if self.world == 4 and self.stage == 4:\n",
        "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
        "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
        "                reward = -50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        if self.world == 4 and self.stage == 2 and done == False and info['y_pos'] >= 255:\n",
        "            reward -= 50\n",
        "        if self.world == 8 and self.stage == 4:\n",
        "            if info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500:\n",
        "                done = True\n",
        "                reward -= 100\n",
        "            if info[\"x_pos\"] >= 3675 and info[\"x_pos\"] <= 3700:\n",
        "                done = True\n",
        "                reward -= 50\n",
        "\n",
        "            if info[\"x_pos\"] < self.max_x - 200:\n",
        "                if self.max_x >= 1250 and self.max_x <= 1310: #solved bug because x_pos duplicated\n",
        "                    if info[\"x_pos\"] >= 320:\n",
        "                        done = True\n",
        "                        reward = -50\n",
        "                elif info[\"x_pos\"] == 312:\n",
        "                    done = True\n",
        "                    reward = -50\n",
        "                elif info[\"x_pos\"] == 56 and self.max_x > 3650 and self.sea_map == False:\n",
        "                    reward += 50\n",
        "                    self.sea_map = True\n",
        "            if info[\"x_pos\"] > self.max_x + 100:\n",
        "                reward += 50\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        self.max_x = max(self.max_x, self.current_x)\n",
        "        self.current_score = info[\"score\"]\n",
        "\n",
        "        return state, reward / 10., done, trunc, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ETgAc_RboE7c",
      "metadata": {
        "id": "ETgAc_RboE7c"
      },
      "source": [
        "# Create MultipleEnvironments\n",
        "MultipleEnvironments use multi-processing to parallel running.\n",
        "\n",
        "Because in the training process, we need to reset the environment when the agent reaches the terminal state. But if we will do it in parallel, then I don't want to check each environment and reset (by loop) or create a new function that parallels check and reset all environments. Then I reset the environment if done = True in step function and set next_state = env.reset(). Then in training, we just set state = next_state (next_state is reset state if done = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2mcbU_JvQwI",
      "metadata": {
        "id": "y2mcbU_JvQwI"
      },
      "outputs": [],
      "source": [
        "#modify from https://github.com/uvipen/Super-mario-bros-PPO-pytorch/blob/master/src/env.py\n",
        "def create_env(world, stage, action_type, test=False):\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", new_step_api=True)\n",
        "    else:\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "    if action_type == \"right\":\n",
        "        action_type = RIGHT_ONLY\n",
        "    elif action_type == \"simple\":\n",
        "        action_type = SIMPLE_MOVEMENT\n",
        "    else:\n",
        "        action_type = COMPLEX_MOVEMENT\n",
        "\n",
        "    env = JoypadSpace(env, action_type)\n",
        "\n",
        "    if test == False:\n",
        "        env = NoopResetEnv(env)\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = CustomRewardAndDoneEnv(env, world, stage)\n",
        "    env = GrayScaleResizeObservation(env, shape=84)\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "    else:\n",
        "        env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "class MultipleEnvironments:\n",
        "    def __init__(self, world, stage, action_type, num_envs):\n",
        "        self.agent_conns, self.env_conns = zip(*[mp.Pipe(duplex=True) for _ in range(num_envs)])\n",
        "        self.envs = [create_env(world, stage, action_type) for _ in range(num_envs)]\n",
        "\n",
        "        for index in range(num_envs):\n",
        "            process = mp.Process(target=self.run, args=(index,))\n",
        "            process.start()\n",
        "            self.env_conns[index].close()\n",
        "\n",
        "    def run(self, index):\n",
        "        self.agent_conns[index].close()\n",
        "        while True:\n",
        "            request, action = self.env_conns[index].recv()\n",
        "            if request == \"step\":\n",
        "                next_state, reward, done, trunc, info = self.envs[index].step(action)\n",
        "                if done:\n",
        "                    next_state = self.envs[index].reset()\n",
        "                self.env_conns[index].send((next_state, reward, done, trunc, info))\n",
        "            elif request == \"reset\":\n",
        "                self.env_conns[index].send(self.envs[index].reset())\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "    def step(self, actions):\n",
        "        [agent_conn.send((\"step\", act)) for agent_conn, act in zip(self.agent_conns, actions)]\n",
        "        next_states, rewards, dones, truncs, infos = zip(*[agent_conn.recv() for agent_conn in self.agent_conns])\n",
        "        return next_states, rewards, dones, truncs, infos\n",
        "\n",
        "    def reset(self):\n",
        "        [agent_conn.send((\"reset\", None)) for agent_conn in self.agent_conns]\n",
        "        states = [agent_conn.recv() for agent_conn in self.agent_conns]\n",
        "        return states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31dycPWmOuOR",
      "metadata": {
        "id": "31dycPWmOuOR"
      },
      "outputs": [],
      "source": [
        "mp.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K6BD9_fSoU_o",
      "metadata": {
        "id": "K6BD9_fSoU_o"
      },
      "source": [
        "# Create memory\n",
        "Memory just save all info we need to train and return all stored info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1736a4f8-d863-4bed-b290-3812f6c43eae",
      "metadata": {
        "id": "1736a4f8-d863-4bed-b290-3812f6c43eae"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, num_envs):\n",
        "        self.num_envs = num_envs\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def save(self, state, action, reward, next_state, done, logit, V):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(next_state)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logits.append(logit)\n",
        "        self.values.append(V)\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.states, self.actions, self.next_states, self.rewards, self.dones, self.logits, self.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7kDyT3WdzOi",
      "metadata": {
        "id": "D7kDyT3WdzOi"
      },
      "source": [
        "## SumTree\n",
        "I copy from [Howuhh prioritized_experience_replay](https://github.com/Howuhh/prioritized_experience_replay/blob/main/memory/buffer.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8rc2hlv7dws6",
      "metadata": {
        "id": "8rc2hlv7dws6"
      },
      "outputs": [],
      "source": [
        "# The ‘sum-tree’ data structure used here is very similar in spirit to the array representation\n",
        "# of a binary heap. However, instead of the usual heap property, the value of a parent node is\n",
        "# the sum of its children. Leaf nodes store the transition priorities and the internal nodes are\n",
        "# intermediate sums, with the parent node containing the sum over all priorities, p_total. This\n",
        "# provides a efficient way of calculating the cumulative sum of priorities, allowing O(log N) updates\n",
        "# and sampling. (Appendix B.2.1, Proportional prioritization)\n",
        "\n",
        "# Additional useful links\n",
        "# Good tutorial about SumTree data structure:  https://adventuresinmachinelearning.com/sumtree-introduction-python/\n",
        "# How to represent full binary tree as array: https://stackoverflow.com/questions/8256222/binary-tree-represented-using-array\n",
        "class SumTree:\n",
        "    def __init__(self, size):\n",
        "        self.nodes = [0] * (2 * size - 1)\n",
        "        self.data = [None] * size\n",
        "\n",
        "        self.size = size\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "\n",
        "    @property\n",
        "    def total(self):\n",
        "        return self.nodes[0]\n",
        "\n",
        "    def update(self, data_idx, value):\n",
        "        idx = data_idx + self.size - 1  # child index in tree array\n",
        "        change = value - self.nodes[idx]\n",
        "\n",
        "        self.nodes[idx] = value\n",
        "\n",
        "        parent = (idx - 1) // 2\n",
        "        while parent >= 0:\n",
        "            self.nodes[parent] += change\n",
        "            parent = (parent - 1) // 2\n",
        "\n",
        "    def add(self, value, data):\n",
        "        self.data[self.count] = data\n",
        "        self.update(self.count, value)\n",
        "\n",
        "        self.count = (self.count + 1) % self.size\n",
        "        self.real_size = min(self.size, self.real_size + 1)\n",
        "\n",
        "    def get(self, cumsum):\n",
        "        assert cumsum <= self.total\n",
        "\n",
        "        idx = 0\n",
        "        while 2 * idx + 1 < len(self.nodes):\n",
        "            left, right = 2*idx + 1, 2*idx + 2\n",
        "\n",
        "            if cumsum <= self.nodes[left]:\n",
        "                idx = left\n",
        "            else:\n",
        "                idx = right\n",
        "                cumsum = cumsum - self.nodes[left]\n",
        "\n",
        "        data_idx = idx - self.size + 1\n",
        "\n",
        "        return data_idx, self.nodes[idx], self.data[data_idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SumTree(nodes={self.nodes.__repr__()}, data={self.data.__repr__()})\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GC5hWUqXd5Hp",
      "metadata": {
        "id": "GC5hWUqXd5Hp"
      },
      "source": [
        "## PrioritizedReplayBuffer\n",
        "\n",
        "I edit from [Howuhh prioritized_experience_replay](https://github.com/Howuhh/prioritized_experience_replay/blob/main/memory/buffer.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2fSjn2-d_Kp",
      "metadata": {
        "id": "y2fSjn2-d_Kp"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, learn_step, state_dim, num_action, buffer_size, eps=1e-2, alpha=0.1, beta=0.1):\n",
        "        self.tree = SumTree(size=buffer_size)\n",
        "\n",
        "        # PER params\n",
        "        self.eps = eps  # minimal priority, prevents zero probabilities\n",
        "        self.alpha = alpha  # determines how much prioritization is used, α = 0 corresponding to the uniform case\n",
        "        self.beta = beta  # determines the amount of importance-sampling correction, b = 1 fully compensate for the non-uniform probabilities\n",
        "        self.max_priority = eps  # priority for new samples, init as eps\n",
        "\n",
        "        # transition: state, action, reward, next_state, done\n",
        "        self.states = np.zeros((buffer_size, learn_step, *state_dim), dtype = np.uint8)\n",
        "        self.actions = np.zeros((buffer_size, learn_step))\n",
        "        self.next_states = np.zeros((buffer_size, learn_step, 1, state_dim[1], state_dim[2]), dtype = np.uint8)\n",
        "        self.rewards = np.zeros((buffer_size, learn_step), dtype = np.float32)\n",
        "        self.dones = np.zeros((buffer_size, learn_step))\n",
        "        self.b_logits = np.zeros((buffer_size, learn_step, num_action), dtype = np.float32)\n",
        "\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "        self.size = buffer_size\n",
        "\n",
        "    def add(self, transition, priority):\n",
        "        state, action, reward, next_state, done, b_logit = transition\n",
        "\n",
        "        # store transition index with maximum priority in sum tree\n",
        "\n",
        "        self.tree.add(self.max_priority, self.count)\n",
        "        self.update_priorities([self.count], [priority])\n",
        "\n",
        "        # store transition in the buffer\n",
        "        self.states[self.count] = state\n",
        "        self.actions[self.count] = action\n",
        "        self.rewards[self.count] = reward\n",
        "        next_state = next_state[:, 3]\n",
        "        next_state = next_state.reshape(next_state.shape[0], 1, next_state.shape[1], next_state.shape[2])\n",
        "        self.next_states[self.count] = next_state\n",
        "        self.dones[self.count] = done\n",
        "        self.b_logits[self.count] = b_logit\n",
        "\n",
        "        # update counters\n",
        "        self.count = (self.count + 1) % self.size\n",
        "        self.real_size = min(self.size, self.real_size + 1)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        assert self.real_size >= batch_size, \"buffer contains less samples than batch size\"\n",
        "\n",
        "        sample_idxs, tree_idxs = [], []\n",
        "        priorities = np.zeros((batch_size,))\n",
        "\n",
        "        # To sample a minibatch of size k, the range [0, p_total] is divided equally into k ranges.\n",
        "        # Next, a value is uniformly sampled from each range. Finally the transitions that correspond\n",
        "        # to each of these sampled values are retrieved from the tree. (Appendix B.2.1, Proportional prioritization)\n",
        "        segment = self.tree.total / batch_size\n",
        "        for i in range(batch_size):\n",
        "            a, b = segment * i, segment * (i + 1)\n",
        "\n",
        "            cumsum = random.uniform(a, b)\n",
        "            # sample_idx is a sample index in buffer, needed further to sample actual transitions\n",
        "            # tree_idx is a index of a sample in the tree, needed further to update priorities\n",
        "            tree_idx, priority, sample_idx = self.tree.get(cumsum)\n",
        "\n",
        "            priorities[i] = priority\n",
        "            tree_idxs.append(tree_idx)\n",
        "            sample_idxs.append(sample_idx)\n",
        "\n",
        "        # Concretely, we define the probability of sampling transition i as P(i) = p_i^α / \\sum_{k} p_k^α\n",
        "        # where p_i > 0 is the priority of transition i. (Section 3.3)\n",
        "        probs = priorities / self.tree.total\n",
        "\n",
        "        # The estimation of the expected value with stochastic updates relies on those updates corresponding\n",
        "        # to the same distribution as its expectation. Prioritized replay introduces bias because it changes this\n",
        "        # distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will\n",
        "        # converge to (even if the policy and state distribution are fixed). We can correct this bias by using\n",
        "        # importance-sampling (IS) weights w_i = (1/N * 1/P(i))^β that fully compensates for the non-uniform\n",
        "        # probabilities P(i) if β = 1. These weights can be folded into the Q-learning update by using w_i * δ_i\n",
        "        # instead of δ_i (this is thus weighted IS, not ordinary IS, see e.g. Mahmood et al., 2014).\n",
        "        # For stability reasons, we always normalize weights by 1/maxi wi so that they only scale the\n",
        "        # update downwards (Section 3.4, first paragraph)\n",
        "        weights = (self.real_size * probs) ** -self.beta\n",
        "\n",
        "        # As mentioned in Section 3.4, whenever importance sampling is used, all weights w_i were scaled\n",
        "        # so that max_i w_i = 1. We found that this worked better in practice as it kept all weights\n",
        "        # within a reasonable range, avoiding the possibility of extremely large updates. (Appendix B.2.1, Proportional prioritization)\n",
        "        weights = weights / weights.max()\n",
        "\n",
        "        states = self.states[sample_idxs]\n",
        "        next_states = self.next_states[sample_idxs]\n",
        "        next_states = np.concatenate((states[:, :, :3, :, :], next_states), axis = 2)\n",
        "\n",
        "        batch = (\n",
        "            states.transpose(1, 0, 2, 3, 4),\n",
        "            self.actions[sample_idxs].transpose(1, 0),\n",
        "            next_states.transpose(1, 0, 2, 3, 4),\n",
        "            self.rewards[sample_idxs].transpose(1, 0),\n",
        "            self.dones[sample_idxs].transpose(1, 0),\n",
        "            self.b_logits[sample_idxs].transpose(1, 0, 2)\n",
        "        )\n",
        "        return batch, weights, tree_idxs\n",
        "\n",
        "    def update_priorities(self, data_idxs, priorities):\n",
        "        if isinstance(priorities, torch.Tensor):\n",
        "            priorities = priorities.detach().cpu().numpy()\n",
        "\n",
        "        for data_idx, priority in zip(data_idxs, priorities):\n",
        "            # The first variant we consider is the direct, proportional prioritization where p_i = |δ_i| + eps,\n",
        "            # where eps is a small positive constant that prevents the edge-case of transitions not being\n",
        "            # revisited once their error is zero. (Section 3.3)\n",
        "            priority = (priority + self.eps) ** self.alpha\n",
        "\n",
        "            self.tree.update(data_idx, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AV0FdMt3oZZY",
      "metadata": {
        "id": "AV0FdMt3oZZY"
      },
      "source": [
        "# Create agent\n",
        "The agent includes 4 main functions:\n",
        "## train\n",
        "train function train agent via many episodes:\n",
        "- Reset the first state.\n",
        "- loop until the agent wins this stage or reaches the maximum episode/step:\n",
        "    - predict value, logit for the current state\n",
        "    - sample action from logit with category distribution (select_action function)\n",
        "    - log all info to memory\n",
        "    - train agent every learn_step (on_policy_learn and off_policy_learn functions)\n",
        "    - eval agent every save_figure_step (save_figure function)\n",
        "    - set state = next_state (I reset environment when agent reach terminal state then next_state is first state if done=True)\n",
        "\n",
        "## select_action\n",
        "this function sample action from logit:\n",
        "- just convert logit to probability: policy = F.softmax(logits, dim=1)\n",
        "- create distribution from probability: distribution = torch.distributions.Categorical(policy)\n",
        "- sample action from distribution: actions = distribution.sample()\n",
        "\n",
        "## save_figure\n",
        "this function eval agent and saves agent/video if the agent yields better total rewards:\n",
        "- reset the environment.\n",
        "- loop until the agent reaches the terminal state.\n",
        "    - predict logit from model\n",
        "    - get action = argmax (logit)\n",
        "    - environment do this action to get next_state, reward, info, done\n",
        "    - if total_reward > best test total reward or agent complete this stage, I save model and video.\n",
        "    - if agent completes this state, we stop training.\n",
        "\n",
        "## on_policy_learn\n",
        "this function trains agent from the experiment saved in memory\n",
        "- get all data from memory\n",
        "- calculate td (lambda) target and gae advantages\n",
        "- train num_epoch epochs:\n",
        "    - shuffle data\n",
        "    - train with each batch data:\n",
        "        - calculate loss\n",
        "        - norm gradient\n",
        "        - update model from loss\n",
        "- push data to PER\n",
        "\n",
        "## off_policy_learn\n",
        "this function trains agent from the experiment sampled from PER\n",
        "- sample data from PER\n",
        "- calculate td (lambda) target and gae advantages\n",
        "- train 1 epochs:\n",
        "    - shuffle data\n",
        "    - train with each batch data:\n",
        "        - calculate loss\n",
        "        - norm gradient\n",
        "        - update model from loss\n",
        "- update priority for PER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-yL01WMumqF",
      "metadata": {
        "id": "d-yL01WMumqF"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, world, stage, action_type, envs, num_envs, state_dim, action_dim, save_dir, save_model_step,\n",
        "                 save_figure_step, learn_step, total_step_or_episode, total_step, total_episode, model, old_model,\n",
        "                 gamma, learning_rate, entropy_coef, V_coef, max_grad_norm,\n",
        "                 clip_param, batch_size, num_epoch, is_normalize_advantage, V_loss_type, target_kl, gae_lambda,\n",
        "                 per_buffer_size, per_eps, per_alpha, per_beta, eps_marg, off_epoch,\n",
        "                 device):\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        self.action_type = action_type\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "        self.learn_step = learn_step\n",
        "        self.total_step_or_episode = total_step_or_episode\n",
        "        self.total_step = total_step\n",
        "        self.total_episode = total_episode\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.current_episode = 0\n",
        "\n",
        "        self.save_model_step = save_model_step\n",
        "        self.save_figure_step = save_figure_step\n",
        "\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = envs\n",
        "        self.model = model.to(self.device)\n",
        "        self.old_model = old_model.to(self.device)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.V_coef = V_coef\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.clip_param = clip_param\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epoch = num_epoch\n",
        "        self.off_epoch = off_epoch\n",
        "\n",
        "        self.per_buffer_size = per_buffer_size\n",
        "        self.per_eps = per_eps\n",
        "        self.per_alpha = per_alpha\n",
        "        self.per_beta = per_beta\n",
        "        self.eps_marg = eps_marg\n",
        "        self.per = PrioritizedReplayBuffer(self.learn_step, self.state_dim, self.action_dim, self.per_buffer_size, eps=self.per_eps, alpha=self.per_alpha, beta=self.per_beta)\n",
        "        self.memory = Memory(self.num_envs)\n",
        "        self.is_completed = False\n",
        "\n",
        "        self.env = None\n",
        "        self.max_test_score = -1e9\n",
        "        self.is_normalize_advantage = is_normalize_advantage\n",
        "        self.V_loss_type = V_loss_type\n",
        "        self.target_kl = target_kl\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        # I just log 1000 lastest update and print it to log.\n",
        "        self.V_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.P_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.E_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.approx_kl_divs = np.zeros((1000,)).reshape(-1)\n",
        "        self.total_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.loss_index = 0\n",
        "        self.len_loss = 0\n",
        "\n",
        "    def save_figure(self, is_training = False):\n",
        "        # test current model and save model/figure if model yield best total rewards.\n",
        "        # create env for testing, reset test env\n",
        "        if self.env is None:\n",
        "            self.env = create_env(self.world, self.stage, self.action_type, True)\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        images = []\n",
        "        total_reward = 0\n",
        "        total_step = 0\n",
        "        num_repeat_action = 0\n",
        "        old_action = -1\n",
        "\n",
        "        episode_time = datetime.now()\n",
        "\n",
        "        # play 1 episode, just get loop action with max probability from model until the episode end.\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                logit, V = self.model(torch.tensor(state, dtype = torch.float, device = self.device).unsqueeze(0))\n",
        "            action = logit.argmax(-1).item()\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            state = next_state\n",
        "            img = Image.fromarray(self.env.current_state)\n",
        "            images.append(img)\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "\n",
        "            if action == old_action:\n",
        "                num_repeat_action += 1\n",
        "            else:\n",
        "                num_repeat_action = 0\n",
        "            old_action = action\n",
        "            if num_repeat_action == 200:\n",
        "                break\n",
        "\n",
        "        #logging, if model yield better result, save figure (test_episode.mp4) and model (best_model.pth)\n",
        "        if is_training:\n",
        "            f_out = open(f\"logging_test.txt\", \"a\")\n",
        "            f_out.write(f'episode_reward: {total_reward:.4f} episode_step: {total_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} approx_kl_div: {(self.approx_kl_divs.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time}\\n')\n",
        "            f_out.close()\n",
        "\n",
        "        if total_reward > self.max_test_score or info['flag_get']:\n",
        "            imageio.mimsave('test_episode.mp4', images)\n",
        "            self.max_test_score = total_reward\n",
        "            if is_training:\n",
        "                torch.save(self.model.state_dict(), f\"best_model.pth\")\n",
        "\n",
        "        # if model can complete this game, stop training by set self.is_completed to True\n",
        "        if info['flag_get']:\n",
        "            self.is_completed = True\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), f\"model_{self.current_step}.pth\")\n",
        "\n",
        "    def load_model(self, model_path = None):\n",
        "        if model_path is None:\n",
        "            model_path = f\"model_{self.current_step}.pth\"\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def update_loss_statis(self, loss_p, loss_v, loss_e, loss, approx_kl_div):\n",
        "        # update loss for logging, just save 1000 latest updates.\n",
        "        self.V_loss[self.loss_index] = loss_v\n",
        "        self.P_loss[self.loss_index] = loss_p\n",
        "        self.E_loss[self.loss_index] = loss_e\n",
        "        self.total_loss[self.loss_index] = loss\n",
        "        self.approx_kl_divs[self.loss_index] = approx_kl_div\n",
        "        self.loss_index = (self.loss_index + 1)%1000\n",
        "        self.len_loss = min(self.len_loss+1, 1000)\n",
        "\n",
        "    def select_action(self, states):\n",
        "        # select action when training, we need use Categorical distribution to make action base on probability from model\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, V = self.model(states)\n",
        "            policy = F.softmax(logits, dim=1).clamp(max=1 - 1e-20)\n",
        "            distribution = torch.distributions.Categorical(policy)\n",
        "            actions = distribution.sample().cpu().numpy().tolist()\n",
        "        return actions, logits, V\n",
        "\n",
        "    def calculate_gae_and_td_target(self, states, actions, next_states, rewards, dones, values = None, b_logits = None):\n",
        "        if values is None:\n",
        "            values = [None] * len(states)\n",
        "            values_ = []\n",
        "            logits = []\n",
        "        if b_logits is None:\n",
        "            b_logits = [None] * len(states)\n",
        "        else:\n",
        "            pro_dones = []\n",
        "            pro_done = 1\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets = []\n",
        "        with torch.no_grad():\n",
        "            _, next_value = self.old_model(torch.tensor(np.array(next_states[-1]), device = self.device))\n",
        "        target = next_value\n",
        "        advantage = 0\n",
        "        advantages = []\n",
        "\n",
        "        for state, action, next_state, reward, done, V, b_logit in zip(states[::-1], actions[::-1], next_states[::-1], rewards[::-1], dones[::-1], values[::-1], b_logits[::-1]):\n",
        "            if V is None:\n",
        "                with torch.no_grad():\n",
        "                    logit, V = self.old_model(torch.tensor(np.array(state), device = self.device))\n",
        "                    values_.append(V)\n",
        "                    logits.append(logit)\n",
        "            if b_logit is not None:\n",
        "                b_logit = torch.tensor(b_logit, device = self.device)\n",
        "                prob = torch.softmax(logit, -1).clamp(max=1 - 1e-20)\n",
        "                b_prob = torch.softmax(b_logit, -1).clamp(max=1 - 1e-20)\n",
        "                action = torch.tensor(np.array(action).reshape(-1)).long()\n",
        "                index = torch.arange(len(prob))\n",
        "                action_prob = prob[index, action]\n",
        "                action_b_prob = b_prob[index, action]\n",
        "                done_ = torch.tensor(done, device = self.device, dtype = torch.double).reshape(-1)\n",
        "                pro_done = torch.ones((1, ), device = self.device).double() * (done_ + (1 - done_) * pro_done) * (action_prob.double() / (action_b_prob + 1e-18).double())\n",
        "                pro_dones.append(pro_done.clone().clamp(max = 1e15).float())\n",
        "                pro_done = pro_done.clamp(max = 1e100) #fix when pro_done very small or very large\n",
        "\n",
        "            done = torch.tensor(done, device = self.device, dtype = torch.float).reshape(-1, 1)\n",
        "            reward = torch.tensor(reward, device = self.device).reshape(-1, 1)\n",
        "\n",
        "            target = next_value * self.gamma * (1-done) + reward\n",
        "            advantage = target + self.gamma * advantage * (1-done) * self.gae_lambda\n",
        "            targets.append(advantage)\n",
        "            advantage = advantage - V.detach()\n",
        "            advantages.append(advantage.detach().view(-1))\n",
        "            next_value = V.detach()\n",
        "\n",
        "        targets = targets[::-1]\n",
        "        advantages = advantages[::-1]\n",
        "\n",
        "        if b_logits[0] is None:\n",
        "            if values[0] is None:\n",
        "                values = values_[::-1]\n",
        "                logits = logits[::-1]\n",
        "                return targets, advantages, logits, values, _\n",
        "            return targets, advantages, _, _, _\n",
        "        else:\n",
        "            pro_dones = pro_dones[::-1]\n",
        "            if values[0] is None:\n",
        "                values = values_[::-1]\n",
        "                logits = logits[::-1]\n",
        "                return targets, advantages, logits, values, pro_dones\n",
        "            return targets, advantages, _, _, pro_dones\n",
        "\n",
        "\n",
        "    def on_policy_learn(self):\n",
        "        # get all data\n",
        "        states, actions, next_states, rewards, dones, old_logits, old_values = self.memory.get_data()\n",
        "\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets, advantages, _, _, _ = self.calculate_gae_and_td_target(states, actions, next_states, rewards, dones, old_values, None)\n",
        "        backup_data = (states, actions, next_states, rewards, dones, old_logits, advantages)\n",
        "\n",
        "        # convert all data to tensor\n",
        "        action_index = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "        states = states.reshape((-1,  states.shape[2], states.shape[3], states.shape[4]))\n",
        "        old_values = torch.cat(old_values, 0)\n",
        "        targets = torch.cat(targets, 0).view(-1, 1)\n",
        "        advantages = torch.cat(advantages, 0).view(-1)\n",
        "        old_logits = torch.cat(old_logits, 0)\n",
        "        old_probs = torch.softmax(old_logits, -1).clamp(max=1 - 1e-20)\n",
        "        index = torch.arange(0, len(old_probs), device = self.device)\n",
        "        old_log_probs = (old_probs[index, action_index] + 1e-9).log()\n",
        "        early_stopping = False\n",
        "\n",
        "        #train num_epoch time\n",
        "        for epoch in range(self.num_epoch):\n",
        "            #shuffle data for each epoch\n",
        "            shuffle_ids = torch.randperm(len(targets), dtype = torch.int64)\n",
        "            for i in range(len(old_values)//self.batch_size):\n",
        "                #train with batch_size data\n",
        "                self.optimizer.zero_grad()\n",
        "                start_id = i * self.batch_size\n",
        "                end_id = min(len(shuffle_ids), (i+1) * self.batch_size)\n",
        "                batch_ids = shuffle_ids[start_id:end_id]\n",
        "\n",
        "                #predict logits and values from model\n",
        "                logits, values = self.model(states[batch_ids])\n",
        "\n",
        "                #calculate entropy and value loss (using mse or huber based on config)\n",
        "                probs =  torch.softmax(logits, -1).clamp(max=1 - 1e-20)\n",
        "                entropy = (- (probs * (probs + 1e-9).log()).sum(-1)).mean()\n",
        "                if self.V_loss_type == 'huber':\n",
        "                    loss_V = F.smooth_l1_loss(values, targets[batch_ids])\n",
        "                else:\n",
        "                    loss_V = F.mse_loss(values, targets[batch_ids])\n",
        "\n",
        "                # calculate log_probs\n",
        "                index = torch.arange(0, len(probs), device = self.device)\n",
        "                batch_action_index = action_index[batch_ids]\n",
        "\n",
        "                log_probs = (probs[index, batch_action_index] + 1e-9).log()\n",
        "\n",
        "                #approx_kl_div copy from https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/ppo/ppo.html#PPO\n",
        "                #if approx_kl_div larger than 1.5 * target_kl (if target_kl in config is not None), stop training because policy change so much\n",
        "                with torch.no_grad():\n",
        "                    log_ratio = log_probs - old_log_probs[batch_ids]\n",
        "                    approx_kl_div = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
        "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                    early_stopping = True\n",
        "\n",
        "                #calculate policy loss\n",
        "                ratio = torch.exp(log_probs - old_log_probs[batch_ids])\n",
        "                batch_advantages = advantages[batch_ids].detach()\n",
        "                if self.is_normalize_advantage:\n",
        "                    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-9)\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages\n",
        "                loss_P = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # update model\n",
        "                loss = - entropy * self.entropy_coef + loss_V * self.V_coef + loss_P\n",
        "\n",
        "                self.update_loss_statis(loss_P.item(), loss_V.item(), entropy.item(), loss.item(), approx_kl_div.item())\n",
        "\n",
        "                if early_stopping == False:\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "                    self.optimizer.step()\n",
        "                else:\n",
        "                    break\n",
        "            if early_stopping:\n",
        "                break\n",
        "\n",
        "        # calculate priority of each trajectory and save it to per\n",
        "        states, actions, next_states, rewards, dones, old_logits, advantages = backup_data\n",
        "        priorities = torch.stack(advantages, 0).abs().mean(0)\n",
        "        states = np.array(states, dtype = np.uint8).transpose(1, 0, 2, 3, 4)\n",
        "        next_states = np.array(next_states, dtype = np.uint8).transpose(1, 0, 2, 3, 4)\n",
        "        actions = np.array(actions).transpose(1, 0)\n",
        "        rewards = np.array(rewards).transpose(1, 0)\n",
        "        dones = np.array(dones).transpose(1, 0)\n",
        "        old_logits = torch.stack(old_logits, 1).cpu().numpy()#.transpose(1, 0, 2)\n",
        "        priorities = priorities.cpu().numpy()\n",
        "        for state, action, reward, next_state, done, old_logit, priority in zip(states, actions, rewards, next_states, dones, old_logits, priorities):\n",
        "            self.per.add((state, action, reward, next_state, done, old_logit), priority)\n",
        "\n",
        "    def off_policy_learn(self):\n",
        "        # get all data\n",
        "        (states, actions, next_states, rewards, dones, b_logits), _, per_index = self.per.sample(self.num_envs)\n",
        "\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets, advantages, old_logits, old_values, pro_dones = self.calculate_gae_and_td_target(states, actions, next_states, rewards, dones, None, b_logits)\n",
        "        backup_data = (states, actions, next_states, rewards, dones, advantages)\n",
        "\n",
        "        # convert all data to tensor\n",
        "        action_index = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "        states = states.reshape((-1,  states.shape[2], states.shape[3], states.shape[4]))\n",
        "        old_values = torch.cat(old_values, 0)\n",
        "        targets = torch.cat(targets, 0).view(-1, 1)\n",
        "        advantages = torch.cat(advantages, 0).view(-1)\n",
        "        old_logits = torch.cat(old_logits, 0)\n",
        "        old_probs = torch.softmax(old_logits, -1).clamp(max=1 - 1e-20)\n",
        "        index = torch.arange(0, len(old_probs), device = self.device)\n",
        "        old_log_probs = (old_probs[index, action_index] + 1e-9).log()\n",
        "        early_stopping = False\n",
        "\n",
        "        b_logits = torch.tensor(b_logits, device = self.device).reshape(-1, b_logits.shape[-1]).float()\n",
        "        b_probs = torch.softmax(b_logits, -1).clamp(max=1 - 1e-20)\n",
        "        b_log_probs = (b_probs[index, action_index] + 1e-9).log()\n",
        "        old_action_probs = old_probs[index, action_index]\n",
        "        b_action_probs = b_probs[index, action_index]\n",
        "        pro_dones = torch.cat(pro_dones, 0)\n",
        "        pro_marg = torch.min(torch.ones(pro_dones.shape, device = self.device) * (1 - self.eps_marg), pro_dones) + torch.relu((pro_dones - (1 - self.eps_marg)) / (pro_dones + 1e-9))\n",
        "        pro_marg = pro_marg.float()\n",
        "        advantages = advantages * pro_marg\n",
        "\n",
        "        #train 1 time\n",
        "        shuffle_ids = torch.randperm(len(targets), dtype = torch.int64)\n",
        "        for i in range(len(old_values)//self.batch_size):\n",
        "                #train with batch_size data\n",
        "                self.optimizer.zero_grad()\n",
        "                start_id = i * self.batch_size\n",
        "                end_id = min(len(shuffle_ids), (i+1) * self.batch_size)\n",
        "                batch_ids = shuffle_ids[start_id:end_id]\n",
        "\n",
        "                #predict logits and values from model\n",
        "                logits, values = self.model(states[batch_ids])\n",
        "\n",
        "                #calculate entropy and value loss (using mse or huber based on config)\n",
        "                probs =  torch.softmax(logits, -1).clamp(max=1 - 1e-20)\n",
        "                entropy = (- (probs * (probs + 1e-9).log()).sum(-1)).mean()\n",
        "\n",
        "                index = torch.arange(0, len(probs), device = self.device)\n",
        "                batch_action_index = action_index[batch_ids]\n",
        "                batch_b_action_probs = b_action_probs[batch_ids].view(-1)\n",
        "                action_probs = probs[index, batch_action_index].view(-1)\n",
        "\n",
        "                if self.V_loss_type == 'huber':\n",
        "                    loss_V =  F.smooth_l1_loss(values, targets[batch_ids], reduction = 'none').view(-1)\n",
        "                else:\n",
        "                    loss_V = F.mse_loss(values, targets[batch_ids], reduction = 'none').view(-1)\n",
        "                loss_V = (pro_marg[batch_ids] * loss_V).mean()\n",
        "\n",
        "                # calculate log_probs\n",
        "                log_probs = (probs[index, batch_action_index] + 1e-9).log()\n",
        "\n",
        "                #approx_kl_div copy from https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/ppo/ppo.html#PPO\n",
        "                #if approx_kl_div larger than 1.5 * target_kl (if target_kl in config is not None), stop training because policy change so much\n",
        "                with torch.no_grad():\n",
        "                    log_ratio = log_probs - old_log_probs[batch_ids]\n",
        "                    approx_kl_div = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
        "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                    early_stopping = True\n",
        "\n",
        "                #calculate policy loss\n",
        "                ratio = torch.exp(log_probs - b_log_probs[batch_ids])\n",
        "                batch_advantages = advantages[batch_ids].detach()\n",
        "                if self.is_normalize_advantage:\n",
        "                    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-9)\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages\n",
        "                loss_P = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # update model\n",
        "                loss = - entropy * self.entropy_coef + loss_V * self.V_coef + loss_P\n",
        "\n",
        "                self.update_loss_statis(loss_P.item(), loss_V.item(), entropy.item(), loss.item(), approx_kl_div.item())\n",
        "\n",
        "                if early_stopping == False:\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "                    self.optimizer.step()\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        # calculate priority of each trajectory and save it to per\n",
        "        states, actions, next_states, rewards, dones, advantages = backup_data\n",
        "        priorities = torch.stack(advantages, 0).abs().mean(0)\n",
        "        priorities = priorities.cpu().numpy()\n",
        "        self.per.update_priorities(per_index, priorities)\n",
        "\n",
        "    def train(self):\n",
        "        episode_reward = [0] * self.num_envs\n",
        "        episode_step = [0] * self.num_envs\n",
        "        max_episode_reward = 0\n",
        "        max_episode_step = 0\n",
        "        episode_time = [datetime.now() for _ in range(self.num_envs)]\n",
        "        total_time = datetime.now()\n",
        "\n",
        "        last_episode_rewards = []\n",
        "\n",
        "        #reset envs\n",
        "        states = self.envs.reset()\n",
        "\n",
        "        while True:\n",
        "            # finish training if agent reach total_step or total_episode base on what type of total_step_or_episode is step or episode\n",
        "            self.current_step += 1\n",
        "\n",
        "            if self.total_step_or_episode == 'step':\n",
        "                if self.current_step >= self.total_step:\n",
        "                    break\n",
        "            else:\n",
        "                if self.current_episode >= self.total_episode:\n",
        "                    break\n",
        "\n",
        "            actions, logits, values = self.select_action(states)\n",
        "\n",
        "            next_states, rewards, dones, truncs, infos = self.envs.step(actions)\n",
        "\n",
        "            # save to memory\n",
        "            self.memory.save(states, actions, rewards, next_states, dones, logits, values)\n",
        "\n",
        "            episode_reward = [x + reward for x, reward in zip(episode_reward, rewards)]\n",
        "            episode_step = [x+1 for x in episode_step]\n",
        "\n",
        "             # logging after each step, if 1 episode is ending, I will log this to logging.txt\n",
        "            for i, done in enumerate(dones):\n",
        "                if done:\n",
        "                    self.current_episode += 1\n",
        "                    max_episode_reward = max(max_episode_reward, episode_reward[i])\n",
        "                    max_episode_step = max(max_episode_step, episode_step[i])\n",
        "                    last_episode_rewards.append(episode_reward[i])\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f'episode: {self.current_episode} agent: {i} rewards: {episode_reward[i]:.4f} steps: {episode_step[i]} complete: {infos[i][\"flag_get\"]==True} mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean():.4f} max_rewards: {max_episode_reward:.4f} max_steps: {max_episode_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} approx_kl_div: {(self.approx_kl_divs.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time[i]} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    episode_reward[i] = 0\n",
        "                    episode_step[i] = 0\n",
        "                    episode_time[i] = datetime.now()\n",
        "\n",
        "            # training agent every learn_step\n",
        "            if self.current_step % self.learn_step == 0:\n",
        "                with torch.no_grad():\n",
        "                    self.old_model.load_state_dict(self.model.state_dict())\n",
        "                self.on_policy_learn()\n",
        "                self.memory.reset()\n",
        "                if self.current_step // self.learn_step >= self.off_epoch:\n",
        "                    for epoch in range(self.off_epoch):\n",
        "                        self.off_policy_learn()\n",
        "\n",
        "            # eval agent every save_figure_step\n",
        "            if self.current_step % self.save_figure_step == 0:\n",
        "                self.save_figure(is_training=True)\n",
        "                if self.is_completed:\n",
        "                    return\n",
        "\n",
        "            if self.current_step % self.save_model_step == 0:\n",
        "                self.save_model()\n",
        "\n",
        "            states = list(next_states)\n",
        "\n",
        "        f_out = open(f\"logging.txt\", \"a\")\n",
        "        f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "        f_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MvF7y-KZ0_2f",
      "metadata": {
        "id": "MvF7y-KZ0_2f"
      },
      "source": [
        "# Create model\n",
        "I use the same model architecture as [PPO](https://github.com/CVHvn/Mario_PPO/blob/main/src/model.py) without LSTM and init weights.\n",
        "Model includes:\n",
        "- 4 convolution layers to encode input image (observation) to feature vector.\n",
        "- 1 hidden linear layer.\n",
        "- two linear layers for policy and value prediction (actor and critic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb",
      "metadata": {
        "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim[0], 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.linear = nn.Linear(1152, 512)\n",
        "        self.critic_linear = nn.Linear(512, 1)\n",
        "        self.actor_linear = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x/255.))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return self.actor_linear(x), self.critic_linear(x)\n",
        "\n",
        "model = Model(config.state_dim, config.action_dim)\n",
        "old_model = Model(config.state_dim, config.action_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wmSXgBKH3TE3",
      "metadata": {
        "id": "wmSXgBKH3TE3"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077a241d-419a-4f72-9f2c-7356d7858eae",
      "metadata": {
        "id": "077a241d-419a-4f72-9f2c-7356d7858eae"
      },
      "outputs": [],
      "source": [
        "envs = MultipleEnvironments(config.world, config.stage, config.action_type, config.num_envs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539",
      "metadata": {
        "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539"
      },
      "outputs": [],
      "source": [
        "agent = Agent(world = config.world, stage = config.stage, action_type = config.action_type, envs = envs, num_envs = config.num_envs,\n",
        "              state_dim = config.state_dim, action_dim = config.action_dim, save_dir = config.save_dir,\n",
        "              save_model_step = config.save_model_step, save_figure_step = config.save_figure_step, learn_step = config.learn_step,\n",
        "              total_step_or_episode = config.total_step_or_episode, total_step = config.total_step, total_episode = config.total_episode,\n",
        "              model = model, old_model = old_model, gamma = config.gamma, learning_rate = config.learning_rate,\n",
        "              entropy_coef = config.entropy_coef, V_coef = config.V_coef,\n",
        "              max_grad_norm = config.max_grad_norm, clip_param = config.clip_param, batch_size = config.batch_size,\n",
        "              num_epoch = config.num_epoch, is_normalize_advantage = config.is_normalize_advantage, V_loss_type = config.V_loss_type,\n",
        "              target_kl = config.target_kl, gae_lambda = config.gae_lambda,  per_buffer_size = config.per_buffer_size,\n",
        "              per_eps = config.per_eps, per_alpha = config.per_alpha, per_beta = config.per_beta, eps_marg = config.eps_marg, off_epoch = config.off_epoch,\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca",
      "metadata": {
        "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca"
      },
      "outputs": [],
      "source": [
        "agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uZNAUSgr3VV-",
      "metadata": {
        "id": "uZNAUSgr3VV-"
      },
      "source": [
        "#test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424dd243-b8f0-4f43-ace5-16d31f190f06",
      "metadata": {
        "id": "424dd243-b8f0-4f43-ace5-16d31f190f06"
      },
      "outputs": [],
      "source": [
        "agent.load_model(\"best_model.pth\")\n",
        "agent.save_figure()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}